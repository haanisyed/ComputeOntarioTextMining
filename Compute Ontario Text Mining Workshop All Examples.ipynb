{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e569a79a",
   "metadata": {},
   "source": [
    "# 2023 COSS Text Mining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cd1bfab",
   "metadata": {},
   "source": [
    " Welcome to Compute Ontario Text Mining Workshop's Jupyter Notebook. The dataset we will be using is called FederalistDataset.xlsx . This dataset has numbered various text excerpts by certain authors and organized into three columns: number, author, text. Text mining is the process of extracting meaning, patterns, and trends from unstructured textual data. Massive amounts of unstructured text are prevalent in today's society. Traditional machine learning algorithms handle only numerical or categorical data. Existing data analytical platforms provide special components to facilitate the analysis of textual data. This workshop introduces the topic of text mining and provides a tour with hands-on exercises and demonstrations of some basic texting mining tools, each of which supports an interesting and diverse set of features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9648828f",
   "metadata": {},
   "source": [
    "## In this Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4f7bc5b",
   "metadata": {},
   "source": [
    " -  Use multiple Python libraries, such as Pandas, NLTK, string, numpy, scipy, textBlob, and sci-kit learn\n",
    " - Preprocessing Text Data using preprocessing techniques (Tokenization, lowercase conversion, lemmatization, stemming, stopword removal)\n",
    " - Text Vectorization using TF-IDF (Term Frequency-Inverse Document Frequency) with Python library (sci-kit learn)\n",
    " - Text Classification using textBlob and Naive-Bayes classifer (supervised machine learning algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d45e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataset to be used is an excel sheet called FederalistDataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #for example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0552b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #for example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db7d3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #for example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69c035fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer #for example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7962cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string #for example 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8631360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #for Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b729d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy #for Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1fb59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer #for Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9705dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #for Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba9e4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer #for Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf73180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer #for Example 3\n",
    "from sklearn.linear_model import LogisticRegression #for Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f3d513c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC #imports support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9ce22d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score #metrics for sentiment analysis after classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fd7d1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #for Naive Bayes Classifier in small exercise in Example 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d04d5a49",
   "metadata": {},
   "source": [
    "### Example 1: Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1b5fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"\\\\Users\\\\haani.admin\\\\Downloads\\\\FederalistDataset.xlsx\" #File path of dataset in my case. Will be slightly different for each person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "844fed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(file_path) #reads excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9209ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number    author                                               text\n",
      "0       1  HAMILTON  To the People of the State of New York <l> AFT...\n",
      "1       2       JAY  To the People of the State of New York <l> WHE...\n",
      "2       3       JAY  To the People of the State of New York <l> IT ...\n",
      "3       4       JAY  To the People of the State of New York <l> MY ...\n",
      "4       5       JAY  To the People of the State of New York <l> QUE...\n"
     ]
    }
   ],
   "source": [
    "print(df.head()) #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60326411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['number', 'author', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns) #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79052c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author'] = df['author'].str.lower() #makes the author column all lowercase using str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "444981dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     hamilton\n",
      "1          jay\n",
      "2          jay\n",
      "3          jay\n",
      "4          jay\n",
      "        ...   \n",
      "80    hamilton\n",
      "81    hamilton\n",
      "82    hamilton\n",
      "83    hamilton\n",
      "84    hamilton\n",
      "Name: author, Length: 85, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['author']) #proof that all of author column is lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84c8a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation))) removing all punctuation from text column of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94fc57b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     To the People of the State of New York l AFTER...\n",
      "1     To the People of the State of New York l WHEN ...\n",
      "2     To the People of the State of New York l IT IS...\n",
      "3     To the People of the State of New York l MY LA...\n",
      "4     To the People of the State of New York l QUEEN...\n",
      "                            ...                        \n",
      "80    To the People of the State of New York l LET U...\n",
      "81    To the People of the State of New York l THE e...\n",
      "82    To the People of the State of New York l THE o...\n",
      "83    To the People of the State of New York l IN TH...\n",
      "84    To the People of the State of New York l ACCOR...\n",
      "Name: text, Length: 85, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(df['text']) #proof that all punctuation got removed from the column called text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fcfc8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57e4de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() #lemmatizer initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1bc18574",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() #stemming initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c1a4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english')) #stopwords initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aab1a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = [] #array to be appended to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77c27993",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df['text']: #for loop which tokenizes, lowercases, lemmatizes, stemms, and does stopword removing of text column\n",
    "    tokens = nltk.word_tokenize(text) #tokenization (removes all punctuation)\n",
    "    tokens = [token.lower() for token in tokens] #lowercase conversion\n",
    "    tokens = [lemmatizer.lemmatize(token)for token in tokens]    #lemmatization\n",
    "    tokens = [stemmer.stem(token) for token in tokens] #Stemming\n",
    "    tokens = [token for token in tokens if token not in stopwords] #stopword removing\n",
    "    preprocessed = ''.join(tokens)\n",
    "    preprocessed_text.append(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6d40b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d801883",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76997c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['text']) #tests if all changes of the for loop were successfully done to text (column in dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90c4d774",
   "metadata": {},
   "source": [
    "### Example 2: Text Vectorization using TF-IDF (Term Frequency-Inverse Document Frequency) with Python Library (sci-kit learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab609a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df['text'].tolist() #converts dataframe into python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ab14e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates an instance of TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4affda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e35b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fits the vectorizer on the text data to learn the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e293707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae3003aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms the text data into TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e2bf15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = vectorizer.transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e96a6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5765950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14578a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code above, df['text'] refers to the text column of the dataFrame 'df'\n",
    "#tolist() is used to convert the column into a list of text data\n",
    "#the remaining steps are for creating TF-IDF vectors which are displayed\n",
    "#in array format: each row corresponds to a document from the 'text' column\n",
    "#each column represents a unique word or term from the vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eabebfa6",
   "metadata": {},
   "source": [
    "### Example 3: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal of this exercise: Implement classification model (one of: Naïve Bayes, Support Vector Machines) using scikit-learn "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd7290ff",
   "metadata": {},
   "source": [
    "#### Example 3: Exercise 1 (using SentimentIntensityAnalyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5ef47409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Apply sentiment analysis using TextBlob\n",
    "df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2e73e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign sentiment labels based on polarity values\n",
    "df['sentiment_label'] = df['sentiment'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2541e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = \"\" #creates a df for sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "69a58eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer() #sia variable created to represent SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e30f769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows(): #sentiment breakdown to be used in a classification model\n",
    "    text = row['text']\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        df.at[index, 'y'] = 'positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        df.at[index, 'y'] = 'negative'\n",
    "    else:\n",
    "        df.at[index, 'y'] = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a5784d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     neutral\n",
      "1     neutral\n",
      "2     neutral\n",
      "3     neutral\n",
      "4     neutral\n",
      "       ...   \n",
      "80    neutral\n",
      "81    neutral\n",
      "82    neutral\n",
      "83    neutral\n",
      "84    neutral\n",
      "Name: y, Length: 85, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['y']) #assigns sentiment labelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "060d246c",
   "metadata": {},
   "source": [
    "#### Example 3: End of Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d2816e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df['text'] splits data into training + testing sets. 80% of data used for training, 20% for testing\n",
    "#y = df['y']\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #X=feature matrix, y=Target Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c7946d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e2a34c0",
   "metadata": {},
   "source": [
    "#### Example 3: Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is just a small exercise demonstrating sentiment analysis library called TextBlob which provides a simple API for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing sentiment analysis on text data. TextBlob's sentiment analysis functionality uses a pre-trained sentiment polarity model based on Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f480ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textblob's sentimental analysis model assigns a polarity score to each text input, indicating sentiment polarity as: Positive (1.0), Negative(-1.0) or Neutral(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d26399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TextBlob's sentiment analysis model utilizes a Naive Bayes classifier trained on a large dataset of movie reviews that have been labeled with sentiment polarity. Capturing patterns in the text data to predict sentiment polarity based on the words and phrases present in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "55954e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob #import statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "485ddc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article #import statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out these three urls and find out what the sentiment score is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://en.wikipedia.org/wiki/Computer_science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6275493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.cnbc.com/2020/06/07/stock-market-futures-open-to-close-news.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6791685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.cnbc.com/2020/04/22/recession-depth-will-be-much-worse-than-2007-2009-lakshman-achuthan.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0659a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Computer_science' #input any of the sample links from above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e0e3c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = Article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b9351dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "db90bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = blob.sentiment.polarity #-1 to 1 where: -1 = Negative, 0 = Neutral, 1 = Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2ef758ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(sentiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a78b87a",
   "metadata": {},
   "source": [
    "#### Example 3: End of Exercise 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
